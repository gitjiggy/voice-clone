import os
import json
import time
import asyncio
import torch
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional
import logging
import librosa
import soundfile as sf
from TTS.api import TTS

logger = logging.getLogger(__name__)

class PretrainedVoiceTrainer:
    """
    Simple voice profile creator using pre-trained TTS models
    Fast and reliable alternative to complex fine-tuning
    """
    
    def __init__(self, base_path: str = "/Users/nirajdesai/Documents/AI/voice-clone"):
        self.base_path = Path(base_path)
        self.data_dir = self.base_path / "data"
        self.processed_dir = self.data_dir / "processed"
        self.models_dir = self.base_path / "models" / "voice_clone"
        self.models_dir.mkdir(parents=True, exist_ok=True)
        
        self.training_status = {
            "status": "idle",
            "progress_pct": 0,
            "eta_min": 0,
            "current_epoch": 0,
            "mode": "idle",
            "error": None
        }
        
        # Load pre-trained TTS model
        self.tts_model = None
    
    def get_training_status(self) -> Dict[str, Any]:
        """Get current training status"""
        # Check if we have a saved completion state
        state_file = self.models_dir / "training_state.json"
        if state_file.exists():
            try:
                with open(state_file) as f:
                    saved_state = json.load(f)
                if saved_state.get("status") == "completed":
                    return {
                        "status": "completed",
                        "progress_pct": 100,
                        "eta_min": 0,
                        "current_epoch": 1,
                        "mode": "completed",
                        "error": None
                    }
            except Exception:
                pass
        
        return self.training_status.copy()
    
    async def analyze_dataset(self) -> Dict[str, Any]:
        """Quick analysis of audio clips"""
        try:
            audio_files = list(self.processed_dir.glob("*.wav"))
            
            if not audio_files:
                return {
                    "total_duration_min": 0,
                    "clip_count": 0,
                    "avg_quality": 0,
                    "recommended_mode": "Need audio clips"
                }
            
            total_duration = 0
            quality_scores = []
            
            for audio_file in audio_files:
                try:
                    # Get duration
                    audio, sr = librosa.load(audio_file, sr=None)
                    duration = len(audio) / sr
                    total_duration += duration
                    
                    # Simple quality score based on duration and audio properties
                    if 15 <= duration <= 30:
                        quality = 85 + min(15, duration - 15)  # 85-100% for good duration
                    else:
                        quality = 60  # Lower for outside optimal range
                        
                    quality_scores.append(quality)
                    
                except Exception as e:
                    logger.warning(f"Error analyzing {audio_file}: {e}")
                    quality_scores.append(50)
            
            avg_quality = np.mean(quality_scores) if quality_scores else 0
            total_duration_min = total_duration / 60
            
            # Write simple manifest
            manifest_path = self.data_dir / "dataset.jsonl"
            with open(manifest_path, "w") as f:
                for i, audio_file in enumerate(audio_files):
                    entry = {
                        "path": str(audio_file),
                        "duration": quality_scores[i] if i < len(quality_scores) else 30,
                        "quality": quality_scores[i] if i < len(quality_scores) else 75
                    }
                    f.write(json.dumps(entry) + "\n")
            
            recommended_mode = "voice_profile" if total_duration_min >= 5 else "need_more_audio"
            
            logger.info(f"ðŸ“Š Dataset analysis: {len(audio_files)} clips, {total_duration_min:.1f}m, {avg_quality:.0f}% quality")
            
            return {
                "total_duration_min": total_duration_min,
                "clip_count": len(audio_files),
                "avg_quality": avg_quality,
                "recommended_mode": recommended_mode
            }
            
        except Exception as e:
            logger.error(f"Dataset analysis failed: {e}")
            return {
                "total_duration_min": 0,
                "clip_count": 0,
                "avg_quality": 0,
                "recommended_mode": "error"
            }
    
    async def start_training(self, max_epochs: int = None, force_mode: Optional[str] = None) -> Dict[str, Any]:
        """Start voice profile creation using pre-trained models"""
        try:
            logger.info("ðŸŽ¯ STARTING VOICE PROFILE CREATION (Pre-trained)")
            
            # Get dataset info
            dataset_info = await self.analyze_dataset()
            total_duration = dataset_info.get("total_duration_min", 0)
            
            # Check if we have enough data
            if total_duration < 5:
                return {
                    "status": "insufficient_data",
                    "mode": "insufficient",
                    "estimated_time_min": 0,
                    "message": "Need at least 5 minutes of audio for voice profile",
                    "total_duration_min": total_duration
                }
            
            # Voice profile creation (much faster than training)
            mode = "voice_profile"
            estimated_time = 3  # 3 minutes for voice profile creation
            
            logger.info(f"ðŸŽ¤ Creating voice profile from {total_duration:.1f} minutes of audio")
            
            # Initialize status
            self.training_status.update({
                "status": "training",
                "progress_pct": 10,
                "eta_min": estimated_time,
                "current_epoch": 0,
                "mode": mode
            })
            
            # Start voice profile creation in background
            asyncio.create_task(self._create_voice_profile(total_duration, estimated_time))
            
            return {
                "status": "training_started",
                "mode": mode,
                "estimated_time_min": estimated_time,
                "max_epochs": 1,
                "dataset_size": len(list(self.processed_dir.glob("*.wav")))
            }
            
        except Exception as e:
            logger.error(f"Failed to start voice profile creation: {e}")
            self.training_status.update({
                "status": "error",
                "error": str(e)
            })
            return {"status": "error", "error": str(e)}
    
    async def _create_voice_profile(self, total_duration: float, estimated_time: int):
        """Create voice profile using pre-trained TTS model"""
        try:
            start_time = time.time()
            
            # Step 1: Load pre-trained TTS model (20% progress)
            logger.info("ðŸ“¦ Loading pre-trained TTS model...")
            self.training_status.update({"progress_pct": 20, "eta_min": estimated_time - 1})
            await asyncio.sleep(2)  # Simulate model loading
            
            self.tts_model = TTS("tts_models/en/ljspeech/tacotron2-DDC")  # Fast, reliable model
            
            # Step 2: Process user audio clips (40% progress)
            logger.info("ðŸŽµ Processing your audio clips...")
            self.training_status.update({"progress_pct": 40, "eta_min": estimated_time - 2})
            
            audio_files = list(self.processed_dir.glob("*.wav"))
            speaker_features = []
            
            for i, audio_file in enumerate(audio_files[:10]):  # Use first 10 clips for speed
                try:
                    # Load and process audio
                    audio, sr = librosa.load(audio_file, sr=22050)
                    
                    # Extract simple speaker features (pitch, energy, etc.)
                    pitches, magnitudes = librosa.piptrack(y=audio, sr=sr)
                    pitch_mean = np.mean(pitches[pitches > 0]) if np.any(pitches > 0) else 200
                    energy = np.mean(librosa.feature.rms(y=audio))
                    
                    speaker_features.append({
                        'pitch': pitch_mean,
                        'energy': float(energy),
                        'duration': len(audio) / sr
                    })
                    
                    # Update progress
                    progress = 40 + (i / len(audio_files[:10])) * 30
                    self.training_status.update({"progress_pct": int(progress)})
                    await asyncio.sleep(0.1)
                    
                except Exception as e:
                    logger.warning(f"Error processing {audio_file}: {e}")
            
            # Step 3: Create voice profile (70% progress)
            logger.info("ðŸ§  Creating your voice profile...")
            self.training_status.update({"progress_pct": 70, "eta_min": 1})
            await asyncio.sleep(1)
            
            # Average speaker characteristics
            avg_pitch = np.mean([f['pitch'] for f in speaker_features]) if speaker_features else 200
            avg_energy = np.mean([f['energy'] for f in speaker_features]) if speaker_features else 0.1
            avg_duration = np.mean([f['duration'] for f in speaker_features]) if speaker_features else 20
            
            # Create voice profile
            voice_profile = {
                'model_type': 'pretrained_tts',
                'base_model': 'tacotron2-DDC',
                'speaker_characteristics': {
                    'average_pitch': float(avg_pitch),
                    'average_energy': float(avg_energy),
                    'speaking_rate': float(avg_duration),
                    'clip_count': len(speaker_features),
                    'total_duration_min': total_duration
                },
                'reference_audio': str(audio_files[0]) if audio_files else None,
                'created_at': time.time(),
                'training_completed': True
            }
            
            # Step 4: Save voice profile (90% progress)
            logger.info("ðŸ’¾ Saving voice profile...")
            self.training_status.update({"progress_pct": 90, "eta_min": 0})
            
            # Save lightweight profile (no heavy model weights)
            profile_path = self.models_dir / "voice_profile.json"
            with open(profile_path, 'w') as f:
                json.dump(voice_profile, f, indent=2)
            
            # Also save config.json for compatibility
            config_path = self.models_dir / "config.json"
            config = {
                "model_type": "pretrained_tts",
                "base_model": "tacotron2-DDC",
                "training_completed": True,
                "voice_profile_ready": True,
                "clip_count": len(speaker_features),
                "total_duration_min": total_duration
            }
            with open(config_path, 'w') as f:
                json.dump(config, f, indent=2)
            
            # Step 5: Generate test output (100% progress)
            logger.info("ðŸŽ¤ Generating test voice sample...")
            await self._generate_test_output(voice_profile)
            
            training_time = (time.time() - start_time) / 60
            logger.info(f"âœ… VOICE PROFILE CREATED in {training_time:.1f} minutes!")
            
            # Mark as completed and keep this state
            self.training_status.update({
                "status": "completed",
                "progress_pct": 100,
                "eta_min": 0,
                "current_epoch": 1,
                "mode": "completed"
            })
            
            # Save completion state so it persists
            completion_state = {
                "status": "completed",
                "voice_profile_ready": True,
                "completed_at": time.time()
            }
            state_file = self.models_dir / "training_state.json"
            with open(state_file, 'w') as f:
                json.dump(completion_state, f)
            
        except Exception as e:
            logger.error(f"âŒ Voice profile creation failed: {e}")
            self.training_status.update({
                "status": "error",
                "error": str(e),
                "progress_pct": 0
            })
    
    async def _generate_test_output(self, voice_profile: Dict[str, Any]):
        """Generate a test audio file with the voice profile"""
        try:
            output_dir = self.base_path / "outputs"
            output_dir.mkdir(exist_ok=True)
            
            # Generate test phrase
            test_text = "Hello, this is my voice profile speaking. The voice conversion is working successfully."
            
            if self.tts_model and voice_profile.get('reference_audio'):
                # Use reference audio for speaker similarity
                reference_audio = voice_profile['reference_audio']
                
                if os.path.exists(reference_audio):
                    # Generate with TTS model
                    output_path = output_dir / "test_voice_profile.wav"
                    
                    # For Tacotron2, we'll generate basic TTS and then post-process
                    # with voice characteristics
                    wav = self.tts_model.tts(text=test_text)
                    
                    # Apply voice characteristics (simple pitch/energy scaling)
                    characteristics = voice_profile['speaker_characteristics']
                    pitch_scale = characteristics['average_pitch'] / 200  # Normalize around 200Hz
                    energy_scale = characteristics['average_energy'] * 5  # Scale energy
                    
                    # Simple voice adaptation (pitch shifting and amplitude scaling)
                    if pitch_scale != 1.0:
                        wav = librosa.effects.pitch_shift(wav, sr=22050, n_steps=np.log2(pitch_scale) * 12)
                    
                    wav = wav * min(energy_scale, 2.0)  # Limit energy scaling
                    
                    # Normalize
                    if np.max(np.abs(wav)) > 0:
                        wav = wav / np.max(np.abs(wav)) * 0.95
                    
                    # Save
                    sf.write(output_path, wav, 22050)
                    logger.info(f"âœ… Test voice sample saved: {output_path}")
                    
        except Exception as e:
            logger.warning(f"Failed to generate test output: {e}")
